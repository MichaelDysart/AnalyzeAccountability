{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tpot AutoML Text Classifier\n",
    "\n",
    "This notebook will show an automated pipeline to implement a classifier, using a package called [Tpot](http://epistasislab.github.io/tpot/). Tpot optimizes sklearn classification pipelines using a genetic algorithm. In particular it automates the feature selection and model selection (including hyperparameter optimization) phases of a machine learning pipeline. The output of the pipeline is the python code for the sklearn pipeline of the best model.\n",
    "\n",
    "This notebook will demonstrate the use of this algorithm, and compare it to the previous results from a simple classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/deap/tools/_hypervolume/pyhv.py:33: ImportWarning: Falling back to the python version of hypervolume module. Expect this to be very slow.\n",
      "  \"module. Expect this to be very slow.\", ImportWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import snowball, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn import svm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tpot import TPOTClassifier\n",
    "from tpot.builtins import StackingEstimator\n",
    "from sklearn.externals.joblib import Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8131, 53)\n",
      "(8127, 53)\n",
      "Index(['StoryID', 'Excerpt', 'CodesApplied_Combined', 'ACCOUNT',\n",
      "       'ACCOUNT_Cultural', 'ACCOUNT_Individual', 'ACCOUNT_Other',\n",
      "       'COMMUNITYRECOVERY', 'EVENT', 'GRIEF', 'GRIEF_Individual',\n",
      "       'GRIEF_Community', 'GRIEF_Societal', 'HERO', 'INVESTIGATION', 'JOURNEY',\n",
      "       'JOURNEY_Mental', 'JOURNEY_Physical', 'LEGAL', 'MEDIA', 'MISCELLANEOUS',\n",
      "       'MOURNING', 'MOURNING_Individual', 'MOURNING_Community',\n",
      "       'MOURNING_Societal', 'PERPETRATOR', 'PHOTO', 'POLICY', 'POLICY_Guns',\n",
      "       'POLICY_InfoSharing', 'POLICY_MentalHealth', 'POLICY_Other',\n",
      "       'POLICY_VictimAdv', 'POLICY_OtherAdv', 'POLICY_Practice',\n",
      "       'PRIVATESECTOR', 'RACECULTURE', 'RESOURCES', 'SAFETY',\n",
      "       'SAFETY_Community', 'SAFETY_Individual', 'SAFETY_SchoolOrg',\n",
      "       'SAFETY_Societal', 'SOCIALSUPPORT', 'THREAT', 'THREAT_Assessment',\n",
      "       'TRAUMA', 'TRAUMA_Physical', 'TRAUMA_Psychological',\n",
      "       'TRAUMA_Individual', 'TRAUMA_Community', 'TRAUMA_Societal', 'VICTIMS'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "file_name = \"Isla Vista - All Excerpts - 1_2_2019.xlsx\"\n",
    "data = pd.read_excel(file_name, sheet_name='Dedoose Excerpts Export')\n",
    "print(data.shape)\n",
    "data = data.dropna(axis=0)\n",
    "print(data.shape)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tokenizer\n",
    "\n",
    "The stemming tokenizer will be used, which preprocesses the text by lowering the case of all words, stems them, removes non-letter characters, and removes stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: A 22-year-old student last Friday killed six people and wounded 13 more in Isla Vista before turning his gun on himself. Commenters \n",
      "blamed the killer�s crimes on everything from misogynistic �pickup artist philosophy� to easy access to guns and no-fault divorce. Even \n",
      "�nerd culture� has come under scrutiny. \n",
      "\n",
      "Is American culture to blame for mass murder? \n",
      "a student last friday kill six peopl and wound more in isla vista before turn his gun on himself comment blame the crime on everyth from misogynist artist to easi access to gun and divorc even has come under scrutini is american cultur to blame for mass murder\n"
     ]
    }
   ],
   "source": [
    "excerpts = list(data['Excerpt'])\n",
    "def stem_tokenizer(doc):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(doc) \n",
    "    stemmer = snowball.SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    list_tokens = [tok.lower() for tok in stemmed_tokens if tok.isalpha()]\n",
    "    return(' '.join(list_tokens))\n",
    "print(\"original: \"+str(excerpts[3]))\n",
    "print(stem_tokenizer(excerpts[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Count Vectorizer\n",
    "\n",
    "The stemmer will be used with the simple count vectorizer, that represents each document as a vector of counts of each word contained in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stem + count\n",
    "docs = [stem_tokenizer(doc) for doc in excerpts]\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "stem_count_X = vectorizer.fit_transform(docs).toarray() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tpot AutoML Classifier\n",
    "\n",
    "### Run 1\n",
    "\n",
    "The count vectors from each document can then be used to test with the autoML tpot pipeline. This is a quick run with only 5 generations to test this method and see if it results in any improvement over the simple classifier implemented previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # the following code takes a couple hours to run\n",
    "# docs_train, docs_test, y_train, y_test = train_test_split(stem_count_X, list(data['ACCOUNT']),\n",
    "#                                                           test_size=0.2, random_state=0) \n",
    "\n",
    "# tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2, config_dict='TPOT sparse',\n",
    "#                      n_jobs = 1, periodic_checkpoint_folder='tpot_checkpoints', cv = 3, memory = 'auto')\n",
    "# tpot.fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Generation 1 - Current best internal CV score: 0.9033967724241256\n",
    "Generation 2 - Current best internal CV score: 0.9033967724241256\n",
    "Generation 3 - Current best internal CV score: 0.9037070657347226\n",
    "Generation 4 - Current best internal CV score: 0.9081677787975023\n",
    "Generation 5 - Current best internal CV score: 0.9081677787975023`\n",
    "\n",
    "`Best pipeline: RandomForestClassifier(BernoulliNB(MultinomialNB(input_matrix, alpha=10.0, fit_prior=True), alpha=1.0, fit_prior=False), bootstrap=False, criterion=gini, max_features=0.15000000000000002, min_samples_leaf=13, min_samples_split=9, n_estimators=100)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9206642066420664\n"
     ]
    }
   ],
   "source": [
    "print(tpot.score(docs_test, np.array(y_test)))\n",
    "tpot.export('tpot_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of Tpot Run 1\n",
    "\n",
    "The final result of the Tpot optimization pipeline is saved in a python file as code that could be used to reproduce that pipeline. This code obtained from the run of tpot is shown in code below. The code was used to fit a model, and the results were analyzed. Note the use of the pre-configured 'Tpot sparse' setting was used, to tailor this optimization pipeline to the sparse data situation, which we have with the vocabulary count vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {'target': list(data['ACCOUNT']), 'data': list(stem_count_X)}\n",
    "tpot_data = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#features = tpot_data.drop('target', axis=1).values\n",
    "features = stem_count_X\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, data['ACCOUNT'].values, random_state=None)\n",
    "\n",
    "# Average CV score on the training set was:0.9081677787975023\n",
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=MultinomialNB(alpha=10.0, fit_prior=True)),\n",
    "    StackingEstimator(estimator=BernoulliNB(alpha=1.0, fit_prior=False)),\n",
    "    RandomForestClassifier(bootstrap=False, criterion=\"gini\", max_features=0.15000000000000002,\n",
    "                           min_samples_leaf=13, min_samples_split=9, n_estimators=100))\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1507   86]\n",
      " [  82  357]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      1593\n",
      "           1       0.81      0.81      0.81       439\n",
      "\n",
      "    accuracy                           0.92      2032\n",
      "   macro avg       0.88      0.88      0.88      2032\n",
      "weighted avg       0.92      0.92      0.92      2032\n",
      "\n",
      "0.9173228346456693\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(testing_target, results))  \n",
    "print(classification_report(testing_target, results))  \n",
    "print(accuracy_score(testing_target, results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results seemed to improve gradually during the 5 generations of the optimization run, however, it has still not reached the level of accuracy obtained by the simple classification model from the previous notebook. Recall the macro average f1 score from simple logistic regression was 0.89, one point higher than the results obtained from the optimized tpot model. \n",
    "\n",
    "### Tpot Run 2\n",
    "\n",
    "To confirm whether the Tpot can result in an improvement, it will be run again for many more generations. Tpot was run again in a python script (outside of this notebook) using the following parameters: \n",
    "`generations=100, population_size=100, verbosity=2, config_dict='TPOT sparse',max_time_mins=1200, max_eval_time_mins=3, scoring = 'f1_macro', n_jobs = 1, periodic_checkpoint_folder='tpot_checkpoints',\n",
    "cv = 5, memory = 'auto'`\n",
    "\n",
    "The parameters changed from the previous run are the number of generations and the population size, which were both increased. Also, the scoring function used to select models was adjusted from accuracy to f1. The output code from this pipeline optimization and results are shown below.\n",
    "\n",
    "### Results of Tpot Run 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1486   80]\n",
      " [  94  372]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94      1566\n",
      "           1       0.82      0.80      0.81       466\n",
      "\n",
      "    accuracy                           0.91      2032\n",
      "   macro avg       0.88      0.87      0.88      2032\n",
      "weighted avg       0.91      0.91      0.91      2032\n",
      "\n",
      "0.9143700787401575\n"
     ]
    }
   ],
   "source": [
    "#features = tpot_data.drop('target', axis=1).values\n",
    "features = stem_count_X\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, data['ACCOUNT'].values, random_state=None)\n",
    "\n",
    "# Average CV score on the training set was:0.8846197491808947\n",
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=LogisticRegression(C=0.01, dual=False, penalty=\"l2\")),\n",
    "    RandomForestClassifier(bootstrap=True, criterion=\"entropy\", max_features=0.2, \n",
    "                           min_samples_leaf=1, min_samples_split=2, n_estimators=100)\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)\n",
    "\n",
    "print(confusion_matrix(testing_target, results))  \n",
    "print(classification_report(testing_target, results))  \n",
    "print(accuracy_score(testing_target, results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "The classification optimization pipeline from Tpot did not result in significant improvements over the results from the simple logistic regression classisier, which had achieved a performance of of 0.89 in macro average, and 0.83 in f1 score for class label 1 (accountability). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
