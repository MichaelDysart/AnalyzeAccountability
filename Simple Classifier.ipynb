{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Text Classifiers\n",
    "\n",
    "This notebook will show a simple approach to text classification. Without any complicated pre-processing, linear and ensemble classification models will be tested. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import snowball, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8131, 53)\n",
      "(8127, 53)\n",
      "Index(['StoryID', 'Excerpt', 'CodesApplied_Combined', 'ACCOUNT',\n",
      "       'ACCOUNT_Cultural', 'ACCOUNT_Individual', 'ACCOUNT_Other',\n",
      "       'COMMUNITYRECOVERY', 'EVENT', 'GRIEF', 'GRIEF_Individual',\n",
      "       'GRIEF_Community', 'GRIEF_Societal', 'HERO', 'INVESTIGATION', 'JOURNEY',\n",
      "       'JOURNEY_Mental', 'JOURNEY_Physical', 'LEGAL', 'MEDIA', 'MISCELLANEOUS',\n",
      "       'MOURNING', 'MOURNING_Individual', 'MOURNING_Community',\n",
      "       'MOURNING_Societal', 'PERPETRATOR', 'PHOTO', 'POLICY', 'POLICY_Guns',\n",
      "       'POLICY_InfoSharing', 'POLICY_MentalHealth', 'POLICY_Other',\n",
      "       'POLICY_VictimAdv', 'POLICY_OtherAdv', 'POLICY_Practice',\n",
      "       'PRIVATESECTOR', 'RACECULTURE', 'RESOURCES', 'SAFETY',\n",
      "       'SAFETY_Community', 'SAFETY_Individual', 'SAFETY_SchoolOrg',\n",
      "       'SAFETY_Societal', 'SOCIALSUPPORT', 'THREAT', 'THREAT_Assessment',\n",
      "       'TRAUMA', 'TRAUMA_Physical', 'TRAUMA_Psychological',\n",
      "       'TRAUMA_Individual', 'TRAUMA_Community', 'TRAUMA_Societal', 'VICTIMS'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "file_name = \"Isla Vista - All Excerpts - 1_2_2019.xlsx\"\n",
    "data = pd.read_excel(file_name, sheet_name='Dedoose Excerpts Export')\n",
    "print(data.shape)\n",
    "data = data.dropna(axis=0)\n",
    "print(data.shape)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tokenizers\n",
    "\n",
    "Two tokenizers will be tested, one with the most simple approach of stemming words. The second has some added complexity, using the WordNet lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: A 22-year-old student last Friday killed six people and wounded 13 more in Isla Vista before turning his gun on himself. Commenters \n",
      "blamed the killer�s crimes on everything from misogynistic �pickup artist philosophy� to easy access to guns and no-fault divorce. Even \n",
      "�nerd culture� has come under scrutiny. \n",
      "\n",
      "Is American culture to blame for mass murder? \n",
      "a student last friday kill six peopl and wound more in isla vista before turn his gun on himself comment blame the crime on everyth from misogynist artist to easi access to gun and divorc even has come under scrutini is american cultur to blame for mass murder\n"
     ]
    }
   ],
   "source": [
    "excerpts = list(data['Excerpt'])\n",
    "def stem_tokenizer(doc):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(doc) \n",
    "    stemmer = snowball.SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    list_tokens = [tok.lower() for tok in stemmed_tokens if tok.isalpha()]\n",
    "    return(' '.join(list_tokens))\n",
    "print(\"original: \"+str(excerpts[3]))\n",
    "print(stem_tokenizer(excerpts[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: \n",
      "A 22-year-old student last Friday killed six people and wounded 13 more in Isla Vista before turning his gun on himself. Commenters \n",
      "blamed the killer�s crimes on everything from misogynistic �pickup artist philosophy� to easy access to guns and no-fault divorce. Even \n",
      "�nerd culture� has come under scrutiny. \n",
      "\n",
      "Is American culture to blame for mass murder? \n",
      "\n",
      "student last friday killed six people wounded isla vista turning gun commenters blamed crime everything misogynistic artist easy access gun divorce even come scrutiny american culture blame mass murder\n"
     ]
    }
   ],
   "source": [
    "excerpts = list(data['Excerpt'])\n",
    "def lem_tokenizer(doc):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(doc) \n",
    "    lemmer = WordNetLemmatizer()\n",
    "    lemmed_tokens = [lemmer.lemmatize(word) for word in tokens if word.lower() not in stop_words]\n",
    "    list_tokens = [tok.lower() for tok in lemmed_tokens if tok.isalpha()]\n",
    "    return(' '.join(list_tokens))\n",
    "print(\"original: \\n\"+str(excerpts[3])+str(\"\\n\"))\n",
    "print(lem_tokenizer(excerpts[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vectorizers\n",
    "\n",
    "The two tokenizers can then be used to create vectorized representation. Two vectorizers will be used. First the count vectorizer, then the tfidf vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stem + count\n",
    "docs = [stem_tokenizer(doc) for doc in excerpts]\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "stem_count_X = vectorizer.fit_transform(docs).toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lem + count\n",
    "docs = [lem_tokenizer(doc) for doc in excerpts]\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "lem_count_X = vectorizer.fit_transform(docs).toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stem + tfidf\n",
    "docs = [stem_tokenizer(doc) for doc in excerpts]\n",
    "vectorizer = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "stem_tfidf_X = vectorizer.fit_transform(docs).toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lem + tfidf\n",
    "docs = [lem_tokenizer(doc) for doc in excerpts]\n",
    "vectorizer = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "lem_tfidf_X = vectorizer.fit_transform(docs).toarray() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Classifiers\n",
    "\n",
    "Test each vectorized representation with simple classifiers.\n",
    "\n",
    "### Linear\n",
    "\n",
    "First compare two linear models: svm and logistic regression\n",
    "\n",
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1177   69]\n",
      " [  66  314]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95      1246\n",
      "           1       0.82      0.83      0.82       380\n",
      "\n",
      "    accuracy                           0.92      1626\n",
      "   macro avg       0.88      0.89      0.88      1626\n",
      "weighted avg       0.92      0.92      0.92      1626\n",
      "\n",
      "0.9169741697416974\n"
     ]
    }
   ],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(stem_count_X, list(data['ACCOUNT']),\n",
    "                                                          test_size=0.2, random_state=0) \n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "#Train the model using the training sets\n",
    "clf.fit(docs_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(docs_test)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  \n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1193   53]\n",
      " [  77  303]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95      1246\n",
      "           1       0.85      0.80      0.82       380\n",
      "\n",
      "    accuracy                           0.92      1626\n",
      "   macro avg       0.90      0.88      0.89      1626\n",
      "weighted avg       0.92      0.92      0.92      1626\n",
      "\n",
      "0.9200492004920049\n"
     ]
    }
   ],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(stem_tfidf_X, list(data['ACCOUNT']),\n",
    "                                                          test_size=0.2, random_state=0) \n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "#Train the model using the training sets\n",
    "clf.fit(docs_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(docs_test)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  \n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1173   73]\n",
      " [  73  307]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94      1246\n",
      "           1       0.81      0.81      0.81       380\n",
      "\n",
      "    accuracy                           0.91      1626\n",
      "   macro avg       0.87      0.87      0.87      1626\n",
      "weighted avg       0.91      0.91      0.91      1626\n",
      "\n",
      "0.9102091020910209\n"
     ]
    }
   ],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(lem_count_X, list(data['ACCOUNT']),\n",
    "                                                          test_size=0.2, random_state=0) \n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "#Train the model using the training sets\n",
    "clf.fit(docs_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(docs_test)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  \n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1188   58]\n",
      " [  75  305]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95      1246\n",
      "           1       0.84      0.80      0.82       380\n",
      "\n",
      "    accuracy                           0.92      1626\n",
      "   macro avg       0.89      0.88      0.88      1626\n",
      "weighted avg       0.92      0.92      0.92      1626\n",
      "\n",
      "0.9182041820418204\n"
     ]
    }
   ],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(lem_tfidf_X, list(data['ACCOUNT']),\n",
    "                                                          test_size=0.2, random_state=0) \n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "#Train the model using the training sets\n",
    "clf.fit(docs_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(docs_test)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  \n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "Since all the setups gave equivalent f1-scores, the simples approach will be chosen to test with logisitic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1185   61]\n",
      " [  68  312]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      1246\n",
      "           1       0.84      0.82      0.83       380\n",
      "\n",
      "    accuracy                           0.92      1626\n",
      "   macro avg       0.89      0.89      0.89      1626\n",
      "weighted avg       0.92      0.92      0.92      1626\n",
      "\n",
      "0.9206642066420664\n"
     ]
    }
   ],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(stem_count_X, list(data['ACCOUNT']), \n",
    "                                                          test_size=0.2, random_state=0) \n",
    "\n",
    "logreg = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "logreg.fit(docs_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(docs_test)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  \n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Model\n",
    "\n",
    "Test the ensemble models random forest.\n",
    "\n",
    "#### Count Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(stem_count_X, list(data['ACCOUNT']), #test_size=0.2,\n",
    "                                                         random_state=0) \n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)  \n",
    "classifier.fit(docs_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1521   38]\n",
      " [ 110  363]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95      1559\n",
      "           1       0.91      0.77      0.83       473\n",
      "\n",
      "    accuracy                           0.93      2032\n",
      "   macro avg       0.92      0.87      0.89      2032\n",
      "weighted avg       0.93      0.93      0.92      2032\n",
      "\n",
      "0.9271653543307087\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(docs_test)  \n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  \n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['studying', 'requires', 'around', 'woodland', 'shifman', 'born', 'men', 'safe', 'month', 'village']\n"
     ]
    }
   ],
   "source": [
    "print(len(classifier.feature_importances_) == len(vectorizer.get_feature_names()))\n",
    "top_feats = np.argsort(classifier.feature_importances_)[-10:]\n",
    "feat_names = [vectorizer.get_feature_names()[feat] for feat in top_feats]\n",
    "print(feat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: [0]\n"
     ]
    }
   ],
   "source": [
    "test_doc = np.zeros(len(docs_test[1]))\n",
    "test_doc[top_feats[6]] = 1\n",
    "print(\"class: \"+str(classifier.predict([test_doc])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensemble model shows the best performance, though it is important to note that the ensemble has a more complex decision boundary, and can be more prone to over-fitting. This would require evaluation through cross-validation to confirm the performance increase. It is also interesting to note the term \"blame\" was one of the top contributing terms to the model, and it results in the decision to classify a document as \"account\" (class label =1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tfidf Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stem + tfidf\n",
    "docs = [stem_tokenizer(doc) for doc in excerpts]\n",
    "vectorizer = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "stem_tfidf_X = vectorizer.fit_transform(docs).toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1523   36]\n",
      " [ 102  371]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96      1559\n",
      "           1       0.91      0.78      0.84       473\n",
      "\n",
      "    accuracy                           0.93      2032\n",
      "   macro avg       0.92      0.88      0.90      2032\n",
      "weighted avg       0.93      0.93      0.93      2032\n",
      "\n",
      "0.9320866141732284\n"
     ]
    }
   ],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(pd.DataFrame(stem_tfidf_X), \n",
    "                                                          list(data['ACCOUNT']), #test_size=0.2,\n",
    "                                                         random_state=0) \n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)  \n",
    "classifier.fit(docs_train, y_train) \n",
    "\n",
    "y_pred = classifier.predict(docs_test)  \n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  \n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['date', 'around', 'woodland', 'requires', 'shifman', 'born', 'safe', 'men', 'month', 'village']\n"
     ]
    }
   ],
   "source": [
    "print(len(classifier.feature_importances_) == len(vectorizer.get_feature_names()))\n",
    "top_feats = np.argsort(classifier.feature_importances_)[-10:]\n",
    "feat_names = [vectorizer.get_feature_names()[feat] for feat in top_feats]\n",
    "print(feat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1177   69]\n",
      " [  66  314]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents incorrectly predicted with label 1\n",
    "incorrect1_ids = [list(docs_test.index)[idx] for idx in range(0,len(docs_test)) \n",
    "              if (y_test[idx]==0 and y_pred[idx] == 1)]\n",
    "incorrect1 = [docs[idx] for idx in incorrect1_ids]\n",
    "\n",
    "# documents incorrectly predicted with label 0\n",
    "incorrect0_ids = [list(docs_test.index)[idx] for idx in range(0,len(docs_test)) \n",
    "              if y_test[idx]==1 and y_pred[idx] == 0]\n",
    "incorrect0 = [docs[idx] for idx in incorrect0_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_ratio = []\n",
    "for i in range(50):\n",
    "    top_features =100\n",
    "    coef = classi.coef_.ravel()\n",
    "    top_positive_coefficients = np.argsort(coef)[-top_features:]\n",
    "    top_words = [list(count_vectorizer.get_feature_names())[idx] for idx in top_positive_coefficients]\n",
    "    toks = incorrect1[i].split(\" \")\n",
    "    account_toks = [word for word in toks if word in top_words]\n",
    "    account_ratio.append(len(account_toks)/(len(toks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['polic', 'offic', 'who', 'spend', 'their', 'day', 'deal', 'with', 'violent', 'troubl', 'peopl', 'describ', 'rodger', 'as', 'polit', 'and', 'courteous'] | ['violent', 'troubl']\n",
      "['elliot', 'rodger', 'bought', 'a', 'handgun', 'in', 'novemb', 'for', 'at', 'a', 'store', 'near', 'his', 'apart', 'he', 'later', 'bought', 'two', 'more'] | ['bought', 'bought']\n",
      "['when', 'polic', 'encount', 'rodger', 'he', 'was', 'calm', 'he', 'was', 'ration', 'he', 'was', 'coher', 'and', 'without', 'other', 'evid', 'there', 'was', 'realli', 'noth', 'they', 'could', 'do', 'skinner', 'said'] | ['evid', 'skinner']\n",
      "['deputi', 'describ', 'rodger', 'as', 'timid', 'and', 'in', 'their', 'interact', 'with', 'him', 'that', 'night'] | ['interact']\n",
      "['we', 'have', 'a', 'session', 'about', 'safeti', 'and', 'risk', 'manag', 'with', 'everi', 'new', 'member', 'she', 'said', 'i', 'think', 'in', 'the', 'futur', 'this', 'will', 'be', 'someth', 'we', 'address'] | ['manag', 'address']\n",
      "['a', 'santa', 'barbara', 'colleg', 'student', 'who', 'felt', 'lone', 'and', 'reject', 'by', 'women', 'unleash', 'his', 'violent', 'retribut', 'on', 'the', 'small', 'seasid', 'town', 'where', 'he', 'live', 'stab', 'and', 'shoot', 'six', 'peopl', 'to', 'death', 'and', 'wound', 'more', 'than', 'a', 'dozen', 'other', 'before', 'appar', 'take', 'his', 'own', 'life'] | ['reject', 'violent', 'small']\n",
      "['this', 'is', 'just', 'about', 'ban', 'gun', 'or', 'knive', 'or', 'anyth', 'els', 'it', 'should', 'be', 'about', 'identifi', 'serious', 'mental', 'ill', 'peopl', 'before', 'the', 'next', 'elliot', 'rodger', 'set', 'out', 'to', 'prove', 'in', 'his', 'word', 'i', 'am', 'in', 'truth', 'the', 'superior', 'one', 'the', 'true', 'alpha', 'male', 'and', 'that', 'good', 'peopl', 'of', 'florida', 'should', 'give', 'you', 'paus', 'florida', 'fund', 'for', 'mental', 'health', 'treatment', 'consist', 'rank', 'among', 'the', 'lowest', 'in', 'the', 'nation'] | ['anyth', 'next', 'word', 'alpha']\n",
      "['ucd', 'student', 'yasmin', 'mortazavi', 'spoke', 'about', 'hong', 'they', 'met', 'in', 'drama', 'class', 'at', 'san', 'jose', 's', 'lynbrook', 'high', 'school', 'in', 'which', 'he', 'work', 'hard', 'to', 'perform', 'shakespear', 'though', 'english', 'wasn', 't', 'his', 'first', 'languag', 'hong', 'felt', 'determin', 'to', 'do', 'well', 'on', 'his', 'final', 'project', 'too', 'only', 'to', 'have', 'his', 'partner', 'go', 'blank', 'onstag', 'unabl', 'to', 'say', 'his', 'line', 'recal', 'mortazavi', 'their', 'scene', 'was', 'realli', 'awkward', 'to', 'watch', 'and', 'our', 'drama', 'teacher', 'who', 'we', 'were', 'all', 'a', 'littl', 'bit', 'scare', 'of', 'didn', 't', 'look', 'very', 'happi', 'at', 'the', 'time', 'this', 'seem', 'like', 'a', 'realli', 'big', 'deal', 'it', 'was', 'their', 'final', 'project', 'and', 'they', 'had', 'fail', 'in', 'front', 'of', 'the', 'entir', 'class', 'afterward', 'jame', 'partner', 'felt', 'so', 'bad', 'that', 'he', 'just', 'put', 'his', 'hand', 'to', 'his', 'face', 'and', 'he', 'start', 'to', 'cri', 'i', 'rememb', 'jame', 'being', 'so', 'sweet', 'he', 'sat', 'next', 'to', 'this', 'guy', 'pat', 'his', 'back', 'tell', 'him', 'it', 'was', 'ok', 'that', 'it', 'wasn', 't', 'a', 'big', 'deal', 'that', 'it', 'wasn', 't', 'his', 'fault', 'and', 'all', 'of', 'us', 'follow', 'suit', 'and', 'start', 'comfort', 'our', 'classmat', 'with', 'jame'] | ['though', 'partner', 'line', 'big', 'fail', 'partner', 'next', 'tell', 'big', 'fault']\n",
      "['rodger', 'kill', 'six', 'student', 'and', 'wound', 'other', 'peopl', 'last', 'month', 'near', 'the', 'univers', 'of', 'california', 'at', 'santa', 'barbara', 'before', 'take', 'his', 'own', 'life', 'he', 'leftbehind', 'video', 'and', 'write', 'express', 'sexual', 'frustrat', 'and', 'reveal', 'his', 'plan', 'to', 'kill', 'women'] | ['frustrat', 'reveal']\n",
      "['provid', 'high', 'school', 'student', 'with', 'greater', 'access', 'to', 'counselor', 'bonamici', 'said', 'presum', 'that', 'would', 'help', 'address', 'mental', 'health', 'issu', 'among', 'youth', 'practic', 'speak', 'focus', 'on', 'the', 'link', 'between', 'gun', 'crime', 'and', 'mental', 'health', 'may', 'be', 'the', 'only', 'area', 'of', 'agreement', 'among', 'member', 'of', 'congress', 'which', 'has', 'fail', 'to', 'act', 'despit', 'unrel', 'mass', 'shoot', 'at', 'school', 'airport', 'militari', 'base', 'and', 'the', 'workplac'] | ['address', 'fail', 'despit']\n"
     ]
    }
   ],
   "source": [
    "example_ids = np.argsort(-np.array(account_ratio))\n",
    "for example_id in example_ids[0:10]:\n",
    "    toks = incorrect1[example_id].split(\" \")\n",
    "    print(str(toks)+\" | \"+str([word for word in toks if word in top_words]))\n",
    "    #print([word for word in toks if word in top_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# documents incorrectly predicted with label 1\n",
    "incorrect1_ids = [list(docs_test.index)[idx] for idx in range(0,len(docs_test)) \n",
    "              if (y_test[idx]==0 and y_pred[idx] == 1)]\n",
    "incorrect1 = [docs[idx] for idx in incorrect1_ids]\n",
    "top_words= ['screen','frustrat','america','notifi','fight','broken','none','dark',\n",
    "            'irrespons','fail','date','count','welfar','role','footbal','brain',\n",
    "            'hornaday','criteria','isol','blame']\n",
    "account_ratio2 = []\n",
    "incorrect1_toks = []\n",
    "for i in range(50):\n",
    "    toks = incorrect1[i].split(\" \")\n",
    "    account_toks = [word for word in toks if word in top_words]\n",
    "    incorrect1_toks.append(account_toks)\n",
    "    account_ratio2.append(len(account_toks)/(len(toks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rodger killed six students and wounded 13 other people last month near the University of California at Santa Barbara before taking his own life. He leftbehind videos and writings expressing sexual frustration and revealing his plans to kill women. \n",
      "['frustrat']\n",
      "****************\n",
      "A separate measure inspired by the Isla Vista tragedy, Senate Bill 505 by Sen. Hannah-Beth Jackson, D-Santa Barbara, was sent to\n",
      "Brown earlier this week.\n",
      "It would require law enforcement agencies to check the Department of Justice’s gun-ownership database whenever they are requested to\n",
      "conduct a welfare check to determine whether the individual may be in possession of a firearm.\n",
      "['welfar']\n",
      "****************\n",
      "Providing high school students with greater access to counselors, Bonamici said. Presumably, that would help address mental\n",
      "health issues among youth. Practically speaking, focusing on the link between gun crimes and mental health may be the only area of\n",
      "agreement among members of Congress, which has failed to act despite unrelenting mass shootings at schools, airports, military bases\n",
      "and the workplace.\n",
      "['fail']\n",
      "****************\n"
     ]
    }
   ],
   "source": [
    "# original docs\n",
    "# documents incorrectly predicted with label 1\n",
    "incorrect1_ids = [list(docs_test.index)[idx] for idx in range(0,len(docs_test)) \n",
    "              if (y_test[idx]==0 and y_pred[idx] == 1)]\n",
    "incorrect1 = [excerpts[idx] for idx in incorrect1_ids]\n",
    "\n",
    "example_ids = np.argsort(-np.array(account_ratio2))\n",
    "for example_id in example_ids[0:3]:\n",
    "    excerpt = incorrect1[example_id]\n",
    "    acc_toks = incorrect1_toks[example_id]\n",
    "    print(excerpt)\n",
    "    print(acc_toks)\n",
    "    print(str(\"****************\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'California has some of the strictest gun laws in the country. When Elliot Rodger\\x92s frantic mother contacted authorities in April, Santa \\nBarbara County sheriff\\x92s deputies went to his apartment but found insufficient evidence to hold him. \\n\\nWhen, as a child, he displayed symptoms of emotional disturbance, his parents found therapists for him. The massacre happened \\nanyway. Nonetheless, the inevitable question looms large as lawmakers, like the rest of us, search for sense in the aftermath of massacre. '"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect0[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "These simple tests show that it should be possible to acheive a fairly high performance classifier, since these very basic pre-processing methods and simple linear classifier were able to achieve a fairly high f-score above 0.9. There is still room for improvement in the classification of the account class label, and one issue that can be seen is the class imbalance. It is not a major imbalance, but it will contribute to the lower f-score for class 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
