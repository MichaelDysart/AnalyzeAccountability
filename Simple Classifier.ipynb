{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Text Classifiers\n",
    "\n",
    "This notebook will show a simple approach to text classification. Without any complicated pre-processing, linear and ensemble classification models will be tested. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import snowball, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8131, 53)\n",
      "(8127, 53)\n",
      "Index(['StoryID', 'Excerpt', 'CodesApplied_Combined', 'ACCOUNT',\n",
      "       'ACCOUNT_Cultural', 'ACCOUNT_Individual', 'ACCOUNT_Other',\n",
      "       'COMMUNITYRECOVERY', 'EVENT', 'GRIEF', 'GRIEF_Individual',\n",
      "       'GRIEF_Community', 'GRIEF_Societal', 'HERO', 'INVESTIGATION', 'JOURNEY',\n",
      "       'JOURNEY_Mental', 'JOURNEY_Physical', 'LEGAL', 'MEDIA', 'MISCELLANEOUS',\n",
      "       'MOURNING', 'MOURNING_Individual', 'MOURNING_Community',\n",
      "       'MOURNING_Societal', 'PERPETRATOR', 'PHOTO', 'POLICY', 'POLICY_Guns',\n",
      "       'POLICY_InfoSharing', 'POLICY_MentalHealth', 'POLICY_Other',\n",
      "       'POLICY_VictimAdv', 'POLICY_OtherAdv', 'POLICY_Practice',\n",
      "       'PRIVATESECTOR', 'RACECULTURE', 'RESOURCES', 'SAFETY',\n",
      "       'SAFETY_Community', 'SAFETY_Individual', 'SAFETY_SchoolOrg',\n",
      "       'SAFETY_Societal', 'SOCIALSUPPORT', 'THREAT', 'THREAT_Assessment',\n",
      "       'TRAUMA', 'TRAUMA_Physical', 'TRAUMA_Psychological',\n",
      "       'TRAUMA_Individual', 'TRAUMA_Community', 'TRAUMA_Societal', 'VICTIMS'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "file_name = \"Isla Vista - All Excerpts - 1_2_2019.xlsx\"\n",
    "data = pd.read_excel(file_name, sheet_name='Dedoose Excerpts Export')\n",
    "print(data.shape)\n",
    "data = data.dropna(axis=0)\n",
    "print(data.shape)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tokenizers\n",
    "\n",
    "Two tokenizers will be tested, one with the most simple approach of stemming words. The second has some added complexity, using the WordNet lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: A 22-year-old student last Friday killed six people and wounded 13 more in Isla Vista before turning his gun on himself. Commenters \n",
      "blamed the killer�s crimes on everything from misogynistic �pickup artist philosophy� to easy access to guns and no-fault divorce. Even \n",
      "�nerd culture� has come under scrutiny. \n",
      "\n",
      "Is American culture to blame for mass murder? \n",
      "a student last friday kill six peopl and wound more in isla vista before turn his gun on himself comment blame the crime on everyth from misogynist artist to easi access to gun and divorc even has come under scrutini is american cultur to blame for mass murder\n"
     ]
    }
   ],
   "source": [
    "excerpts = list(data['Excerpt'])\n",
    "def stem_tokenizer(doc):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(doc) \n",
    "    stemmer = snowball.SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    list_tokens = [tok.lower() for tok in stemmed_tokens if tok.isalpha()]\n",
    "    return(' '.join(list_tokens))\n",
    "print(\"original: \"+str(excerpts[3]))\n",
    "print(stem_tokenizer(excerpts[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: \n",
      "A 22-year-old student last Friday killed six people and wounded 13 more in Isla Vista before turning his gun on himself. Commenters \n",
      "blamed the killer�s crimes on everything from misogynistic �pickup artist philosophy� to easy access to guns and no-fault divorce. Even \n",
      "�nerd culture� has come under scrutiny. \n",
      "\n",
      "Is American culture to blame for mass murder? \n",
      "\n",
      "student last friday killed six people wounded isla vista turning gun commenters blamed crime everything misogynistic artist easy access gun divorce even come scrutiny american culture blame mass murder\n"
     ]
    }
   ],
   "source": [
    "excerpts = list(data['Excerpt'])\n",
    "def lem_tokenizer(doc):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(doc) \n",
    "    lemmer = WordNetLemmatizer()\n",
    "    lemmed_tokens = [lemmer.lemmatize(word) for word in tokens if word.lower() not in stop_words]\n",
    "    list_tokens = [tok.lower() for tok in lemmed_tokens if tok.isalpha()]\n",
    "    return(' '.join(list_tokens))\n",
    "print(\"original: \\n\"+str(excerpts[3])+str(\"\\n\"))\n",
    "print(lem_tokenizer(excerpts[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vectorizers\n",
    "\n",
    "The two tokenizers can then be used to create vectorized representation. Two vectorizers will be used. First the count vectorizer, then the tfidf vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stem + count\n",
    "docs = [stem_tokenizer(doc) for doc in excerpts]\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "stem_count_X = vectorizer.fit_transform(docs).toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lem + count\n",
    "docs = [lem_tokenizer(doc) for doc in excerpts]\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "lem_count_X = vectorizer.fit_transform(docs).toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stem + tfidf\n",
    "docs = [stem_tokenizer(doc) for doc in excerpts]\n",
    "vectorizer = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "stem_tfidf_X = vectorizer.fit_transform(docs).toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lem + tfidf\n",
    "docs = [lem_tokenizer(doc) for doc in excerpts]\n",
    "vectorizer = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "lem_tfidf_X = vectorizer.fit_transform(docs).toarray() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Classifiers\n",
    "\n",
    "Test each vectorized representation with simple classifiers.\n",
    "\n",
    "### Linear\n",
    "\n",
    "First compare two linear models: svm and logistic regression\n",
    "\n",
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1177   69]\n",
      " [  66  314]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95      1246\n",
      "           1       0.82      0.83      0.82       380\n",
      "\n",
      "    accuracy                           0.92      1626\n",
      "   macro avg       0.88      0.89      0.88      1626\n",
      "weighted avg       0.92      0.92      0.92      1626\n",
      "\n",
      "0.9169741697416974\n"
     ]
    }
   ],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(stem_count_X, list(data['ACCOUNT']),\n",
    "                                                          test_size=0.2, random_state=0) \n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "#Train the model using the training sets\n",
    "clf.fit(docs_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(docs_test)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  \n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1193   53]\n",
      " [  77  303]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95      1246\n",
      "           1       0.85      0.80      0.82       380\n",
      "\n",
      "    accuracy                           0.92      1626\n",
      "   macro avg       0.90      0.88      0.89      1626\n",
      "weighted avg       0.92      0.92      0.92      1626\n",
      "\n",
      "0.9200492004920049\n"
     ]
    }
   ],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(stem_tfidf_X, list(data['ACCOUNT']),\n",
    "                                                          test_size=0.2, random_state=0) \n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "#Train the model using the training sets\n",
    "clf.fit(docs_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(docs_test)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  \n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1173   73]\n",
      " [  73  307]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94      1246\n",
      "           1       0.81      0.81      0.81       380\n",
      "\n",
      "    accuracy                           0.91      1626\n",
      "   macro avg       0.87      0.87      0.87      1626\n",
      "weighted avg       0.91      0.91      0.91      1626\n",
      "\n",
      "0.9102091020910209\n"
     ]
    }
   ],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(lem_count_X, list(data['ACCOUNT']),\n",
    "                                                          test_size=0.2, random_state=0) \n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "#Train the model using the training sets\n",
    "clf.fit(docs_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(docs_test)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  \n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1188   58]\n",
      " [  75  305]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95      1246\n",
      "           1       0.84      0.80      0.82       380\n",
      "\n",
      "    accuracy                           0.92      1626\n",
      "   macro avg       0.89      0.88      0.88      1626\n",
      "weighted avg       0.92      0.92      0.92      1626\n",
      "\n",
      "0.9182041820418204\n"
     ]
    }
   ],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(lem_tfidf_X, list(data['ACCOUNT']),\n",
    "                                                          test_size=0.2, random_state=0) \n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "#Train the model using the training sets\n",
    "clf.fit(docs_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(docs_test)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  \n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "Since all the setups gave equivalent f1-scores, the simples approach will be chosen to test with logisitic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1185   61]\n",
      " [  68  312]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      1246\n",
      "           1       0.84      0.82      0.83       380\n",
      "\n",
      "    accuracy                           0.92      1626\n",
      "   macro avg       0.89      0.89      0.89      1626\n",
      "weighted avg       0.92      0.92      0.92      1626\n",
      "\n",
      "0.9206642066420664\n"
     ]
    }
   ],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(stem_count_X, list(data['ACCOUNT']), \n",
    "                                                          test_size=0.2, random_state=0) \n",
    "\n",
    "logreg = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "logreg.fit(docs_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(docs_test)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  \n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Model\n",
    "\n",
    "Test the ensemble models random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(stem_count_X, list(data['ACCOUNT']), #test_size=0.2,\n",
    "                                                         random_state=0) \n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)  \n",
    "classifier.fit(docs_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1521   38]\n",
      " [ 110  363]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95      1559\n",
      "           1       0.91      0.77      0.83       473\n",
      "\n",
      "    accuracy                           0.93      2032\n",
      "   macro avg       0.92      0.87      0.89      2032\n",
      "weighted avg       0.93      0.93      0.92      2032\n",
      "\n",
      "0.9271653543307087\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(docs_test)  \n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  \n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['student', 'reject', 'april', 'women', 'sex', 'blame', 'manifesto', 'rodger', 'mental', 'video']\n"
     ]
    }
   ],
   "source": [
    "print(len(classifier.feature_importances_) == len(vectorizer.get_feature_names()))\n",
    "top_feats = np.argsort(classifier.feature_importances_)[-10:]\n",
    "feat_names = [vectorizer.get_feature_names()[feat] for feat in top_feats]\n",
    "print(feat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: [0]\n"
     ]
    }
   ],
   "source": [
    "test_doc = np.zeros(len(docs_test[1]))\n",
    "test_doc[top_feats[6]] = 1\n",
    "print(\"class: \"+str(classifier.predict([test_doc])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensemble model shows the best performance, though it is important to note that the ensemble has a more complex decision boundary, and can be more prone to over-fitting. This would require evaluation through cross-validation to confirm the performance increase. It is also interesting to note the term \"blame\" was one of the top contributing terms to the model, and it results in the decision to classify a document as \"account\" (class label =1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "These simple tests show that it should be possible to acheive a fairly high performance classifier, since these very basic pre-processing methods and simple linear classifier were able to achieve a fairly high f-score above 0.9. There is still room for improvement in the classification of the account class label, and one issue that can be seen is the class imbalance. It is not a major imbalance, but it will contribute to the lower f-score for class 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
